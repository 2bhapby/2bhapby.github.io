---
layout: post
title: 앙상블 학습과 랜덤 포레스트
---

# 앙상블 학습과 랜덤 포레스트

wisdom of the crowd와 비슷하게 일련의 예측기로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있을 것이다.

일련의 예측기를 **앙상블**이라고 부른다. 그렇기 때문에 이를 **앙상블 학습(ensemble learning)** 이라고 하며 앙상블 학습 알고리즘을 **앙상블 방법(ensemble method)** 이라고 한다.

**앙상블 방법**

- 훈련 세트로부터 무작위로 각기 다른 서브셋을 만들어 일련의 결정 트리 분류기를 훈련시킨다.
- 예측은 모든 개별 트리의 예측을 구하면 된다.
- 그 후 가장 많은 선택을 받은 클래스를 예측으로 삼는다.
- 결정 트리의 앙상블을 **랜덤 포레스트** 라고 한다.
-- 가장 강력한 ML 알고리즘 중 하나
- 예로는 **배깅, 부스팅, 스태킹**이 있다.


## 투표 기반 분류기

**직접 투표 분류기**
- 여러 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 것
-- 다수결 투표로 정해지는 것
- 각 분류기가 **약한 학습기** 라도 충분하게 많고 다양하면 **강한 학습기**가 될 수 있다.
- 이것이 가능한 이유는 **큰 수의 법칙(law of large numbers)** 때문이다.
- 앙상블 방법은 예측기가 가능한 서로 독립적일 때 최고의 성능을 발휘한다.

**간접 투표 분류기**
- 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측한다.
- 확률이 높은 투표에 비중을 더 두기 때문에 직접 투표 방식 보다 성능이 높다.
- 이를 사용하기 위해서는 voting="hard"를 voting="soft" 로 바꾸고 모든 분류기가 클래스의 확률을 추정할 수 있으면 된다.

## 배깅과 페이스팅

알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습시킨다.

**샘플링 방식**
- 배깅(bagging)
-- 훈련세트에서 중복을 허용하는 방식
-- 한 예측기를 위해 같은 훈련 샘플을 여러번 샘플링 할 수 있다.

- 페이스팅(pasting)
-- 중복을 허용하지 않는 방식

모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아서 새로운 샘플에 대한 예측을 만든다.
- 수집함수는 전형적인 분류일 ㄸ
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ1MzgyNTgyOSwxODU0MTc4MDk0LDk2Mz
c2MTQ0MywtNjU2ODcxNzg2LC0xOTUzODQzMDQ4LC0xMzg1MTUz
MzAyLDE3MzM2NDY5OTIsNTc0MjEwNzE4LDkyNzQwODA2OCwtMT
IyMTIyNzgwMCwtMTE5Mjk5NTcxOV19
-->