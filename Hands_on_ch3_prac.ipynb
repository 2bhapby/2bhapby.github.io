{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hands-on_ch3_prac.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOazlqXx+L1wkW+V2Hmtkwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2bhapby/2bhapby.github.io/blob/master/Hands_on_ch3_prac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3gbgSsW7hUK"
      },
      "source": [
        "##1. MNIST 데이터셋으로 분류기를 만들어 테스트 세트에서 97% 정확도를 달성 \r\n",
        "\r\n",
        ">(KNeighborsClassifier 사용, weights, n_neighbors 그리드 탐색으로 시도)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lm6hbq17keC"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.datasets import fetch_openml\r\n",
        "mnist = fetch_openml('mnist_784', version = 1)\r\n",
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Rye4Qg_kqK"
      },
      "source": [
        "X, y = mnist[\"data\"], mnist[\"target\"]\r\n",
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNmOfEVI9CBA"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "\r\n",
        "param_grid = [{'weights' : ['uniform', 'distance'], 'n_neighbors' : [3,4,5]}]\r\n",
        "\r\n",
        "knn_clf = KNeighborsClassifier()\r\n",
        "\r\n",
        "grid_search = GridSearchCV(knn_clf, param_grid, cv = 3, verbose = 3)\r\n",
        "grid_search.fit(X_train, y_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzOH_P_qEwJp"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCf0_g_mE_zz"
      },
      "source": [
        "grid_search.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5-2aDycFDUz"
      },
      "source": [
        "from sklearn.metrics import accuacy_score\r\n",
        "\r\n",
        "y_pred = grid_search.predict(X_test)\r\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFfZxTezFY0S"
      },
      "source": [
        "##2. 데이터 증식(training set expansion)\r\n",
        "\r\n",
        ">MNIST 이미지를 어느 방향으로든 한 픽셀 이동시킬 수 있는 함수를 만들어보세요. \r\n",
        "\r\n",
        ">그런 다음 훈련 세트에 있는 각 이미지에 대해 네 개의 이동된 복사본을 만들어 훈련 세트에 추가하세요. \r\n",
        "\r\n",
        ">마지막으로 이 확장된 데이터셋에서 앞에서 차은 최선의 모델을 훈련시키고 테스트 세트에서 정확도를 측정해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc2JUNTsGmyj"
      },
      "source": [
        "from scipy.ndimage.interpolation import shift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sYsJ4r4Gwe3"
      },
      "source": [
        "def shift_image(image, dx, dy):\r\n",
        "    image = image.reshape((28,28))\r\n",
        "    shifted_image = shift(image, [dx, dy], cval = 0, mode = 'constant')\r\n",
        "    return shifted_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEkYPo1gHz4T"
      },
      "source": [
        "image = X_train[1000]\r\n",
        "shifted_image_up = shift_image(image, 0, 1)\r\n",
        "shifted_image_down = shift_image(image, 0, -1)\r\n",
        "shifted_image_left = shift_image(image, -1, 0)\r\n",
        "shifted_image_right = shift_image(image, 1, 0)\r\n",
        "\r\n",
        "plt.figure(figsize=(12,3))\r\n",
        "plt.subplot(131)\r\n",
        "plt.title(\"Original\", fontsize=14)\r\n",
        "plt.imshow(image.reshape(28,28), interpolation='nearest', cmap='Greys')\r\n",
        "plt.subplot(132)\r\n",
        "plt.title(\"Original\", fontsize=14)\r\n",
        "plt.imshow(shifted_image_up.reshape(28,28), interpolation='nearest', cmap='Greys')\r\n",
        "plt.subplot(133)\r\n",
        "plt.title(\"Original\", fontsize=14)\r\n",
        "plt.imshow(shifted_image_down.reshape(28,28), interpolation='nearest', cmap='Greys')\r\n",
        "plt.subplot(134)\r\n",
        "plt.title(\"Original\", fontsize=14)\r\n",
        "plt.imshow(shifted_image_left.reshape(28,28), interpolation='nearest', cmap='Greys')\r\n",
        "plt.subplot(135)\r\n",
        "plt.title(\"Original\", fontsize=14)\r\n",
        "plt.imshow(shifted_image_right.reshape(28,28), interpolation='nearest', cmap='Greys')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z7wHsT2RqqJ"
      },
      "source": [
        "X_train_augmented = [image for image in X_train]\r\n",
        "y_train_augmented = [label for image in y_train]\r\n",
        "\r\n",
        "for dx, dy in ((1,0),(-1,0),(0,1),(0,-1)):\r\n",
        "    X_train_augmented.append(shift(image, dx, dy))\r\n",
        "    y_train_augmented.append(label)\r\n",
        "\r\n",
        "X_train_augmented = np.array(X_train_augmented)\r\n",
        "y_train_augmented = np.array(y_train_augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ik3nextTIQM"
      },
      "source": [
        "shuffle_idx = np.random.permutation(len(X_train_augmented))\r\n",
        "X_train_augmented = X_train_augmented[shuffle_idx]\r\n",
        "y_train_augmented = y_train_augmented[shuffle_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93PdFs_RTU5C"
      },
      "source": [
        "knn_clf = KNeighborsClassifier(**grid_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzZXO6uvTbqm"
      },
      "source": [
        "knn_clf.fit(X_train_augmented, y_train_augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCsHdY0FTgNx"
      },
      "source": [
        "y_pred = knn_clf.predict(X_test)\r\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1G4_UZnXYWj"
      },
      "source": [
        "##3. 타이타닉(Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXawFGsvJ3o4"
      },
      "source": [
        "따로 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXyrMMWFHwpR"
      },
      "source": [
        "#스팸 분류기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0KprIOxHzqc"
      },
      "source": [
        "- https://homl.info/spamassassin 에서 샘플 다운로드  \r\n",
        "\r\n",
        "- 훈련 세트, 테스트 세트 분리  \r\n",
        "\r\n",
        "- 각 메일을 특성 벡터로 변환하는 데이터 준비 파이프라인 준비\r\n",
        "> 이 파이프 라인은 하나의 이메일을 가능한 단어의 존재 여부를 나타내는 희소벡터로 바꿔야함 (포함 유무 혹은 포함 횟수)\r\n",
        "- 이메일 헤더 제거, 소문자 변환, 구두점 제거, 모든 URLs 주소를 'URL'로 대체, 모든 숫자를 'NUMBER'로 대체, 어간 추출(즉, 단어의 끝을 떼어냄) 등의 수행할지 여부를 제어하기 위한 하이퍼파라미터 추가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bByoiaqTMMGj"
      },
      "source": [
        "###Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt-n_m9gHf37"
      },
      "source": [
        "import os\r\n",
        "import tarfile\r\n",
        "import urllib.request\r\n",
        "\r\n",
        "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\r\n",
        "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\r\n",
        "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\r\n",
        "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\r\n",
        "\r\n",
        "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\r\n",
        "    if not os.path.isdir(spam_path):\r\n",
        "        os.makedirs(spam_path)\r\n",
        "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\r\n",
        "        path = os.path.join(spam_path, filename)\r\n",
        "        if not os.path.isfile(path):\r\n",
        "            urllib.request.urlretrieve(url, path)\r\n",
        "        tar_bz2_file = tarfile.open(path)\r\n",
        "        tar_bz2_file.extractall(path=SPAM_PATH)\r\n",
        "        tar_bz2_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x73nmNQaNLwx"
      },
      "source": [
        "fetch_spam_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-n4FUzsNOUf"
      },
      "source": [
        "ham_dir = os.path.join(SPAM_PATH, 'easy_ham')\r\n",
        "spam_dir = os.path.join(SPAM_PATH, 'spam')\r\n",
        "ham_filenames = [name for name in sorted(os.listdir(ham_dir)) if len(name) > 20]\r\n",
        "spam_filenames = [name for name in sorted(os.listdir(spam_dir)) if len(name) > 20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSzp6dTgPTi3"
      },
      "source": [
        "len(ham_filenames), len(spam_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU7pZuXWPbHv"
      },
      "source": [
        "import email\r\n",
        "import email.policy\r\n",
        "\r\n",
        "def load_email(is_spam, filename, spam_path = SPAM_PATH):\r\n",
        "    directory = 'spam' if is_spam else 'easy_ham'\r\n",
        "    with open(os.path.join(spam_path, directory, filename), 'rb') as f:\r\n",
        "        return email.parser.BytesParser(policy = email.policy.default).parse(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDfnkT6KPxTb"
      },
      "source": [
        "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\r\n",
        "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qVdLOvOQNeE"
      },
      "source": [
        "print(ham_emails[1].get_content().strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeLU-ZQCR94-"
      },
      "source": [
        "print(spam_emails[6].get_content().strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGmAFXifWVew"
      },
      "source": [
        "메일의 형식은 이미지, 첨부파일을 갖는 multipart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wbg_CciUuEx"
      },
      "source": [
        "def get_email_structure(email):\r\n",
        "    if isinstance(email, str):\r\n",
        "        return email\r\n",
        "    payload = email.get_payload()\r\n",
        "    if isinstance(payload, list):\r\n",
        "        return \"multipart({})\".format(\", \".join([\r\n",
        "            get_email_structure(sub_email)\r\n",
        "            for sub_email in payload\r\n",
        "        ]))\r\n",
        "    else:\r\n",
        "        return email.get_content_type()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpdBWi7EUvhk"
      },
      "source": [
        "from collections import Counter\r\n",
        "\r\n",
        "def structures_counter(emails):\r\n",
        "    structures = Counter()\r\n",
        "    for email in emails:\r\n",
        "        structure = get_email_structure(email)\r\n",
        "        structures[structure] += 1\r\n",
        "    return structures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m4bdngQWcr0"
      },
      "source": [
        "structures_counter(ham_emails).most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67CqpAJnWoo1"
      },
      "source": [
        "structures_counter(spam_emails).most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvWvsDx7WsC9"
      },
      "source": [
        "ham email의 경우 대부분 plain text, pgp 서명 존재  \r\n",
        "spam email의 경우 html의 비중이 꽤나 높음, pgp 서명 없음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYbKEIQNXL_S"
      },
      "source": [
        "for header, value in spam_emails[0].items():\r\n",
        "    print(header, \":\", value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbWm52gSXZy9"
      },
      "source": [
        "#####훈련, 테스트 세트 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unRpJD2xSSZr"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X = np.array(ham_emails + spam_emails, dtype = object)\r\n",
        "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHgTZkbIXilv"
      },
      "source": [
        "#####데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nujKSK5VTt4i"
      },
      "source": [
        "import re\r\n",
        "from html import unescape\r\n",
        "\r\n",
        "def html_to_plain_text(html):\r\n",
        "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\r\n",
        "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\r\n",
        "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\r\n",
        "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\r\n",
        "    return unescape(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVAHSraXUROl"
      },
      "source": [
        "html_spam_emails = [email for email in X_train[y_train == 1]\r\n",
        "                    if get_email_structure(email) == \"text/html\"]\r\n",
        "sample_html_spam = html_spam_emails[7]\r\n",
        "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbfmLZtHXy0r"
      },
      "source": [
        "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE9ozLZvZRDM"
      },
      "source": [
        "def email_to_text(email):\r\n",
        "    html = None\r\n",
        "    for part in email.walk():\r\n",
        "        ctype = part.get_content_type()\r\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\r\n",
        "            continue\r\n",
        "        try:\r\n",
        "            content = part.get_content()\r\n",
        "        except:\r\n",
        "            content = str(part.get_payload())\r\n",
        "        if ctype == 'text/plain':\r\n",
        "            return content\r\n",
        "        else:\r\n",
        "            html = content\r\n",
        "    if html:\r\n",
        "        return html_to_plain_text(html)\r\n",
        "\r\n",
        "print(email_to_text(sample_html_spam)[:100], \"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLk4oUFWZ-3W"
      },
      "source": [
        "#####어간 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmTp83v6aAGK"
      },
      "source": [
        "try:\r\n",
        "    import nltk\r\n",
        "\r\n",
        "    stemmer = nltk.PorterStemmer() #어간추출\r\n",
        "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\r\n",
        "        print(word, \"=>\", stemmer.stem(word))\r\n",
        "except ImportError:\r\n",
        "    print(\"Error\")\r\n",
        "    stemmer = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwwh0hv1abPK"
      },
      "source": [
        "try:\r\n",
        "    import google.colab\r\n",
        "    !pip install -q -U urlextract\r\n",
        "\r\n",
        "except ImportError:\r\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqcIdXQ0am5H"
      },
      "source": [
        "#url 바꾸는데 urlextract 사용\r\n",
        "\r\n",
        "try:\r\n",
        "    import urlextract \r\n",
        "\r\n",
        "    url_extractor = urlextract.URLExtract()\r\n",
        "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\r\n",
        "except ImportError:\r\n",
        "    print(\"Error: replacing URLs requires the urlextract module.\")\r\n",
        "    url_extractor = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhypz31bjtl"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "\r\n",
        "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\r\n",
        "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\r\n",
        "                 replace_urls=True, replace_numbers=True, stemming=True):\r\n",
        "        self.strip_headers = strip_headers\r\n",
        "        self.lower_case = lower_case\r\n",
        "        self.remove_punctuation = remove_punctuation\r\n",
        "        self.replace_urls = replace_urls\r\n",
        "        self.replace_numbers = replace_numbers\r\n",
        "        self.stemming = stemming\r\n",
        "\r\n",
        "    def fit(self, X, y=None):\r\n",
        "        return self\r\n",
        "    \r\n",
        "    def transform(self, X, y=None):\r\n",
        "        X_transformed = []\r\n",
        "        for email in X:\r\n",
        "            text = email_to_text(email) or ''\r\n",
        "            if self.lower_case:\r\n",
        "                text = text.lower()\r\n",
        "            if self.replace_urls and url_extractor is not None:\r\n",
        "                urls = list(set(url_extractor.find_urls(text)))\r\n",
        "                urls.sort(key = lambda url: len(url), reverse=True)\r\n",
        "                for url in urls:\r\n",
        "                    text = text.replace(url, \" URL \")\r\n",
        "            if self.replace_numbers:\r\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\r\n",
        "            if self.remove_punctuation:\r\n",
        "                text = re.sub(r'\\W+',' ', text, flags = re.M)\r\n",
        "            word_counts = Counter(text.split())\r\n",
        "            if self.stemming and stemmer is not None:\r\n",
        "                stemmed_word_counts = Counter()\r\n",
        "                for word, count in word_counts.items():\r\n",
        "                    stemmed_word = stemmer.stem(word)\r\n",
        "                    stemmed_word_counts[stemmed_word] += count\r\n",
        "                word_counts = stemmed_word_counts\r\n",
        "            X_transformed.append(word_counts)\r\n",
        "        return np.array(X_transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLi_6EHGiOB_"
      },
      "source": [
        "X_few = X_train[:3]\r\n",
        "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\r\n",
        "X_few_wordcounts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbXX9fshjeU8"
      },
      "source": [
        "단어 카운트를 벡터로 변환하는 transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR_mGj0hjM0f"
      },
      "source": [
        "from scipy.sparse import csr_matrix\r\n",
        "\r\n",
        "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\r\n",
        "    \r\n",
        "    def __init__(self, vocabulary_size = 1000):\r\n",
        "        self.vocabulary_size = vocabulary_size\r\n",
        "\r\n",
        "    def fit(self, X, y=None):\r\n",
        "        total_count = Counter()\r\n",
        "        for word_count in X:\r\n",
        "            for word, count in word_count.items():\r\n",
        "                total_count[word] += min(count, 10)\r\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\r\n",
        "        self.most_common_ = most_common\r\n",
        "        self.vocabulary_ = {word:index + 1 for index, (word, count) in enumerate(most_common)}\r\n",
        "        return self\r\n",
        "\r\n",
        "    def transform(self, X, y = None):\r\n",
        "        rows = []\r\n",
        "        cols = []\r\n",
        "        data = []\r\n",
        "\r\n",
        "        for row, word_count in enumerate(X):\r\n",
        "            for word, count in word_count.items():\r\n",
        "                rows.append(row)\r\n",
        "                cols.append(self.vocabulary_.get(word, 0))\r\n",
        "                data.append(count)\r\n",
        "        return csr_matrix((data, (rows, cols)), shape = (len(X), self.vocabulary_size + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4XUYahrlsB4"
      },
      "source": [
        "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\r\n",
        "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\r\n",
        "X_few_vectors.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJC8Vcxil7Gf"
      },
      "source": [
        "vocab_transformer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX7V5VVNmN4R"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "\r\n",
        "preprocess_pipeline = Pipeline([\r\n",
        "        ('email_to_wordcount', EmailToWordCounterTransformer()),\r\n",
        "        ('wordcount_to_vector', WordCounterToVectorTransformer())\r\n",
        "])\r\n",
        "\r\n",
        "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B10prUqzmpk2"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "log_clf = LogisticRegression(solver = 'lbfgs', max_iter = 1000, random_state=42)\r\n",
        "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\r\n",
        "score.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsneadDsnAs7"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score\r\n",
        "\r\n",
        "X_test_transformed = preprocess_pipeline.transform(X_test)\r\n",
        "\r\n",
        "log_clf = LogisticRegression(solver = 'lbfgs', max_iter=1000, random_state=42)\r\n",
        "log_clf.fit(X_train_transformed, y_train)\r\n",
        "\r\n",
        "y_pred = log_clf.predict(X_test_transformed)\r\n",
        "\r\n",
        "print(\"Precision: {:.2f}%\".format(100*precision_score(y_test, y_pred)))\r\n",
        "print(\"Recall: {:.2f}%\".format(100*recall_score(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}